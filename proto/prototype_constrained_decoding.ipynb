{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a sketch of the constrained decoding algorithm by Hokamp & Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedListWithKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imagine a max_source_len x constraint_len+1 grid\n",
    "# at the top left corner, there is a triangle with constraint_len-1 sides cut out\n",
    "# at the bottom right corner, there is a triangle with constraint_len sides cut out\n",
    "\n",
    "# we move to the right, filling the beams in each column starting with the bottommost, and moving upwards\n",
    "#        - filling the beams in a column can be done in parallel, since there are no dependencies within the column\n",
    "\n",
    "# the horizontal (t) axis represents time\n",
    "#     - every hypothesis in a column t has the same number of tokens\n",
    "# the vertical axis (j) represents coverage of constraints\n",
    "#     - every hypothesis in a row j covers the same number of constraint tokens\n",
    "\n",
    "# FILLING CELL (i,j)\n",
    "# there are two source beams from which we can generate hypotheses:\n",
    "# LEFT (cell (i-1, j))\n",
    "#    - this cell can only generate \n",
    "# BELOW+LEFT (cell (i-1, j-1))\n",
    "#    - this cell can add constraints in two ways:\n",
    "#      (1) constraints which are unfinished _MUST_ be continued\n",
    "#      (2) new constraints can be started\n",
    "#    - hypotheses from this beam always update the constraint coverage\n",
    "\n",
    "# Generating constraint hypotheses\n",
    "# the hypothesis object holds all of the states needed to generate the n-best continuations at the next timestep\n",
    "# this is the purpuse of the `payload` attribute of hypothesis objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# without a special feature, generating a word and using the same word from a constraint have the same score,\n",
    "# thus we need a way to decide whether we are generating a word, or starting a new constraint which begins with \n",
    "# that word\n",
    "# - the constraint pointer model is one way of scoring hypotheses from the different sources differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "sample_constraints = [\n",
    "    [1,2],\n",
    "    [5,6,7]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thinking buffer\n",
    "def init_coverage(constraints):\n",
    "    coverage = []\n",
    "    for c in constraints:\n",
    "        coverage.append(np.zeros(len(c), dtype='int16'))\n",
    "    return coverage\n",
    "\n",
    "def sequence_from_hyp(hyp):\n",
    "    sequence = []\n",
    "    current_hyp = hyp\n",
    "    while current_hyp.backpointer is not None:\n",
    "        sequence.append(current_hyp.token)\n",
    "        current_hyp = current_hyp.backpointer\n",
    "    sequence.append(current_hyp.token)\n",
    "    return sequence[::-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConstraintHypothesis:\n",
    "    \"\"\"A (partial) hypothesis which maintains an additional constraint coverage object\n",
    "    \n",
    "    Args:\n",
    "        token (unicode): the surface form of this hypothesis\n",
    "        score (float): the score of this hypothesis (higher is better)\n",
    "        coverage (list of lists): a representation of the area of the constraints covered by this hypothesis\n",
    "        constraints (list of lists): the constraints that may be used with this hypothesis\n",
    "        payload (:obj:): additional data that comes with this hypothesis. Functions may \n",
    "            require certain data to be present in the payload, such as the previous states, glimpses, etc...\n",
    "        backpointer (:obj:`ConstraintHypothesis`): a pointer to the hypothesis object which generated this one\n",
    "        unfinished_constraint (bool): a flag which indicates whether this hyp is inside an unfinished constraint\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, score, coverage, constraints, payload=None, backpointer=None,\n",
    "                 unfinished_constraint=False):\n",
    "        self.token = token\n",
    "        self.score = score\n",
    "        self.coverage = coverage\n",
    "        self.constraints = constraints\n",
    "        self.backpointer = backpointer\n",
    "        self.payload = payload\n",
    "        self.unfinished_constraint = unfinished_constraint\n",
    "        \n",
    "    def __str__(self):\n",
    "        return u'\\{token: {}\\, score: {}, coverage: {}, constraints: {} \\}'.format(\n",
    "            self.token, self.score, self.coverage, self.constraints)\n",
    "        \n",
    "class AbstractBeam():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # note: here we assume bigger scores are better\n",
    "        self.hypotheses = SortedListWithKey(key=lambda x: -x['score'])\n",
    "        self.size = size\n",
    "    \n",
    "    def add(self, hyp):\n",
    "        self.hypotheses.add(hyp)\n",
    "        if len(self.hypotheses) > self.size:\n",
    "            assert len(self.hypotheses) == self.size + 1\n",
    "            del self.hypotheses[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for hyp in self.hypotheses:\n",
    "            yield hyp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generation_hyps(beam, hyp_generation_func):\n",
    "    \"\"\"return all hyps which are continuations of the hyps on this beam\n",
    "    \n",
    "    hyp_generation_func maps `(hyp) --> continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is not modified in each child\n",
    "    \"\"\"\n",
    "    \n",
    "    continuations = (hyp_generation_func(hyp) for hyp in beam if not hyp.unfinished_constraint)\n",
    "    \n",
    "    # flatten\n",
    "    return (new_hyp for hyp_list in continuations for new_hyp in hyp_list)\n",
    "\n",
    "        \n",
    "def get_new_constraint_hyps(beam, constraints, constraint_hyp_func):\n",
    "    \"\"\"return all hyps which start a new constraint from the hyps on this beam\n",
    "    \n",
    "    constraint_hyp_func maps `(hyp, constraints) --> continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is modified in each child\n",
    "    \"\"\"\n",
    "    \n",
    "    continuations = (constraint_hyp_func(hyp, constraints)\n",
    "                     for hyp in beam if not hyp.unfinished_constraint)\n",
    "    \n",
    "    # flatten\n",
    "    return (new_hyp for hyp_list in continuations for new_hyp in hyp_list)\n",
    "\n",
    "\n",
    "def get_continued_constraint_hyps(beam, constraints, constraint_hyp_func):\n",
    "    \"\"\"return all hyps which continue the unfinished constraints on this beam\n",
    "    \n",
    "    constraint_hyp_func maps `(hyp, constraints) --> forced_continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is modified in each child\n",
    "\n",
    "    \"\"\"\n",
    "    continuations = (constraint_hyp_func(hyp, constraints)\n",
    "                     for hyp in beam if hyp.unfinished_constraint)\n",
    "    \n",
    "    return continuations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Beam():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the implementations of the generate, start_constraint, and continue_constraint functions depend upon the decoder\n",
    "\n",
    "# to generate, we query the decoder with a hypothesis\n",
    "# the decoder uses the payload to compute the next most probable continuations\n",
    "\n",
    "# to start new constrained hypotheses, we need to:\n",
    "# constrained_hyps = []\n",
    "# for constraint_idx in constraint_candidates(hyp):\n",
    "#     new_hyp = build_hyp(hyp, constraint_idx, constraints)\n",
    "#     constrained_hyps.append(new_hyp)\n",
    "\n",
    "\n",
    "# to continue a constrained hypothesis, we need to:\n",
    "# (1) find the constraint to continue via the coverage object\n",
    "# (2) find the next token in the constraint\n",
    "# (3) \n",
    "# (4) get the data for this token (score, states, etc...) -- i.e. forced decode for one step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DUMBEST POSSIBLE IMPLEMENTATION of generation functions\n",
    "# Note that generation and search are done by _different_ classes\n",
    "\n",
    "class DumbDecoder(object):\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "#         def __init__(self, token, score, coverage, constraints, payload=None, backpointer=None,\n",
    "#                  unfinished_constraint=False):\n",
    "    \n",
    "    def dumb_generate(self, hyp, n_best=1):\n",
    "        # make k_best random hyp objects\n",
    "        next_tokens = np.random.choice(self.vocabulary, size=n_best)\n",
    "        next_scores = np.random.random(size=n_best)\n",
    "        \n",
    "        new_hyps = []\n",
    "        for i in range(n_best):\n",
    "            new_hyp = ConstraintHypothesis(token=next_tokens[i],\n",
    "                                           score=next_scores[i],\n",
    "                                           coverage=copy.deepcopy(hyp.coverage),\n",
    "                                           constraints=hyp.constraints,\n",
    "                                           payload=None,\n",
    "                                           backpointer=hyp,\n",
    "                                           unfinished_constraint=False\n",
    "                                          )\n",
    "            new_hyps.append(new_hyp)\n",
    "        \n",
    "        return new_hyps\n",
    "    \n",
    "    def dumb_generate_from_constraints(self, hyp, constraints):\n",
    "        pass\n",
    "    \n",
    "    def dumb_continue_unfinished_constraints(self, hyp, constraints):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_TOKEN = u'<S>'\n",
    "P_START = 1.0\n",
    "N_BEST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dumb_decoder = DumbDecoder(vocabulary)\n",
    "start_hyp = ConstraintHypothesis(token=START_TOKEN, score=P_START,\n",
    "                                 coverage=init_coverage(sample_constraints),\n",
    "                                 constraints=sample_constraints,\n",
    "                                 payload=None,\n",
    "                                 backpointer=None,\n",
    "                                 unfinished_constraint=False\n",
    "                                )\n",
    "next_hyps = dumb_decoder.dumb_generate(start_hyp, n_best=N_BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'<S>', 3], [u'<S>', 0], [u'<S>', 6], [u'<S>', 3], [u'<S>', 5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sequence_from_hyp(h) for h in next_hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.ConstraintHypothesis instance at 0x7f313c0563b0>,\n",
       " <__main__.ConstraintHypothesis instance at 0x7f313c056440>,\n",
       " <__main__.ConstraintHypothesis instance at 0x7f313c056488>,\n",
       " <__main__.ConstraintHypothesis instance at 0x7f313c0564d0>,\n",
       " <__main__.ConstraintHypothesis instance at 0x7f313c056518>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[h for h in next_hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "u'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f25a5285722e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_hyps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a40ed758fa3b>\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         return u'\\{token: {}\\, score: {}, coverage: {}, constraints: {} \\}'.format(\n\u001b[0;32m---> 28\u001b[0;31m             self.token, self.score, self.coverage, self.constraints)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAbstractBeam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: u'token'"
     ]
    }
   ],
   "source": [
    "print(next_hyps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#     def __init__(self, token, score, coverage, constraints, payload=None, backpointer=None,\n",
    "#                  unfinished_constraint=False):\n",
    "start_hyp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hyp(previous_hyp, next_token=None, constraint_idx=None, constraint_token_idx=None)\n",
    "    \"\"\"Utility function to create a hypothesis in different ways\"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
