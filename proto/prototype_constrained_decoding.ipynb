{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a sketch of the constrained decoding algorithm by Hokamp & Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedListWithKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imagine a max_source_len x constraint_len+1 grid\n",
    "# at the top left corner, there is a triangle with constraint_len-1 sides cut out\n",
    "# at the bottom right corner, there is a triangle with constraint_len sides cut out\n",
    "\n",
    "# we move to the right, filling the beams in each column starting with the bottommost, and moving upwards\n",
    "#        - filling the beams in a column can be done in parallel, since there are no dependencies within the column\n",
    "\n",
    "# the horizontal (t) axis represents time\n",
    "#     - every hypothesis in a column t has the same number of tokens\n",
    "# the vertical axis (j) represents coverage of constraints\n",
    "#     - every hypothesis in a row j covers the same number of constraint tokens\n",
    "\n",
    "# FILLING CELL (i,j)\n",
    "# there are two source beams from which we can generate hypotheses:\n",
    "# LEFT (cell (i-1, j))\n",
    "#    - this cell can only generate \n",
    "# BELOW+LEFT (cell (i-1, j-1))\n",
    "#    - this cell can add constraints in two ways:\n",
    "#      (1) constraints which are unfinished _MUST_ be continued\n",
    "#      (2) new constraints can be started\n",
    "#    - hypotheses from this beam always update the constraint coverage\n",
    "\n",
    "# Generating constraint hypotheses\n",
    "# the hypothesis object holds all of the states needed to generate the n-best continuations at the next timestep\n",
    "# this is the purpuse of the `payload` attribute of hypothesis objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# without a special feature, generating a word and using the same word from a constraint have the same score,\n",
    "# thus we need a way to decide whether we are generating a word, or starting a new constraint which begins with \n",
    "# that word\n",
    "# - the constraint pointer model is one way of scoring hypotheses from the different sources differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "sample_constraints = [\n",
    "    [1,2],\n",
    "    [5,6,7]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thinking buffer\n",
    "def init_coverage(constraints):\n",
    "    coverage = []\n",
    "    for c in constraints:\n",
    "        coverage.append(np.zeros(len(c), dtype='int16'))\n",
    "    return coverage\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConstraintHypothesis:\n",
    "    \"\"\"A (partial) hypothesis which maintains an additional constraint coverage object\n",
    "    \n",
    "    Args:\n",
    "        token (unicode): the surface form of this hypothesis\n",
    "        score (float): the score of this hypothesis (higher is better)\n",
    "        coverage (list of lists): a representation of the area of the constraints covered by this hypothesis\n",
    "        constraints (list of lists): the constraints that may be used with this hypothesis\n",
    "        payload (:obj:): additional data that comes with this hypothesis. Functions may \n",
    "            require certain data to be present in the payload, such as the previous states, glimpses, etc...\n",
    "        backpointer (:obj:`ConstraintHypothesis`): a pointer to the hypothesis object which generated this one\n",
    "        constraint_index (tuple): if this hyp is part of a constraint, the index into `self.constraints` which\n",
    "            is covered by this hyp `(constraint_idx, token_idx)`\n",
    "        unfinished_constraint (bool): a flag which indicates whether this hyp is inside an unfinished constraint\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, score, coverage, constraints, payload=None, backpointer=None,\n",
    "                 constraint_index=None, unfinished_constraint=False):\n",
    "        self.token = token\n",
    "        self.score = score\n",
    "        \n",
    "        assert len(coverage) == len(constraints), 'constraints and coverage length must match'\n",
    "        assert all(len(cov) == len(cons) for cov, cons in zip(coverage, constraints)), \\\n",
    "            'each coverage and constraint vector must match'\n",
    "        \n",
    "        self.coverage = coverage\n",
    "        self.constraints = constraints\n",
    "        self.backpointer = backpointer\n",
    "        self.payload = payload\n",
    "        self.constraint_index = constraint_index\n",
    "        self.unfinished_constraint = unfinished_constraint\n",
    "        \n",
    "    def __str__(self):\n",
    "        return u'token: {}, score: {}, coverage: {}, constraints: {}'.format(\n",
    "            self.token, self.score, self.coverage, self.constraints)\n",
    "    \n",
    "    def constraint_candidates(self):\n",
    "        available_constraints = []\n",
    "        for idx in range(len(self.coverage)):\n",
    "            if self.coverage[idx][0] == 0:\n",
    "                available_constraints.append(idx)\n",
    "            \n",
    "        return available_constraints\n",
    "    \n",
    "    def get_sequence(self):\n",
    "        sequence = []\n",
    "        current_hyp = self\n",
    "        while current_hyp.backpointer is not None:\n",
    "            sequence.append(current_hyp.token)\n",
    "            current_hyp = current_hyp.backpointer\n",
    "        sequence.append(current_hyp.token)\n",
    "        return sequence[::-1]\n",
    "        \n",
    "        \n",
    "class AbstractBeam():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # note: here we assume bigger scores are better\n",
    "        self.hypotheses = SortedListWithKey(key=lambda x: -x['score'])\n",
    "        self.size = size\n",
    "    \n",
    "    def add(self, hyp):\n",
    "        self.hypotheses.add(hyp)\n",
    "        if len(self.hypotheses) > self.size:\n",
    "            assert len(self.hypotheses) == self.size + 1\n",
    "            del self.hypotheses[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for hyp in self.hypotheses:\n",
    "            yield hyp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generation_hyps(beam, hyp_generation_func):\n",
    "    \"\"\"return all hyps which are continuations of the hyps on this beam\n",
    "    \n",
    "    hyp_generation_func maps `(hyp) --> continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is not modified in each child\n",
    "    \"\"\"\n",
    "    \n",
    "    continuations = (hyp_generation_func(hyp) for hyp in beam if not hyp.unfinished_constraint)\n",
    "    \n",
    "    # flatten\n",
    "    return (new_hyp for hyp_list in continuations for new_hyp in hyp_list)\n",
    "\n",
    "        \n",
    "def get_new_constraint_hyps(beam, constraints, constraint_hyp_func):\n",
    "    \"\"\"return all hyps which start a new constraint from the hyps on this beam\n",
    "    \n",
    "    constraint_hyp_func maps `(hyp, constraints) --> continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is modified in each child\n",
    "    \"\"\"\n",
    "    \n",
    "    continuations = (constraint_hyp_func(hyp, constraints)\n",
    "                     for hyp in beam if not hyp.unfinished_constraint)\n",
    "    \n",
    "    # flatten\n",
    "    return (new_hyp for hyp_list in continuations for new_hyp in hyp_list)\n",
    "\n",
    "\n",
    "def get_continued_constraint_hyps(beam, constraints, constraint_hyp_func):\n",
    "    \"\"\"return all hyps which continue the unfinished constraints on this beam\n",
    "    \n",
    "    constraint_hyp_func maps `(hyp, constraints) --> forced_continuations`\n",
    "    \n",
    "    the coverage vector of the parent hyp is modified in each child\n",
    "\n",
    "    \"\"\"\n",
    "    continuations = (constraint_hyp_func(hyp, constraints)\n",
    "                     for hyp in beam if hyp.unfinished_constraint)\n",
    "    \n",
    "    return continuations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Beam():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the implementations of the generate, start_constraint, and continue_constraint functions depend upon the decoder\n",
    "\n",
    "# to generate, we query the decoder with a hypothesis\n",
    "# the decoder uses the payload to compute the next most probable continuations\n",
    "\n",
    "# to start new constrained hypotheses, we need to:\n",
    "# constrained_hyps = []\n",
    "# for constraint_idx in constraint_candidates(hyp):\n",
    "#     new_hyp = build_hyp(hyp, constraint_idx, constraints)\n",
    "#     constrained_hyps.append(new_hyp)\n",
    "\n",
    "\n",
    "# to continue a constrained hypothesis, we need to:\n",
    "# (1) find the constraint to continue via the coverage object\n",
    "# (2) find the next token in the constraint\n",
    "# (3) \n",
    "# (4) get the data for this token (score, states, etc...) -- i.e. forced decode for one step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DUMBEST POSSIBLE IMPLEMENTATION of generation functions\n",
    "# Note that generation and search are done by _different_ classes\n",
    "\n",
    "class DumbTranslationModel(object):\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "    \n",
    "    def dumb_generate(self, hyp, n_best=1):\n",
    "        # make k_best random hyp objects\n",
    "        next_tokens = np.random.choice(self.vocabulary, size=n_best)\n",
    "        next_scores = np.random.random(size=n_best)\n",
    "        \n",
    "        new_hyps = []\n",
    "        for i in range(n_best):\n",
    "            new_hyp = ConstraintHypothesis(token=next_tokens[i],\n",
    "                                           score=next_scores[i],\n",
    "                                           coverage=copy.deepcopy(hyp.coverage),\n",
    "                                           constraints=hyp.constraints,\n",
    "                                           payload=None,\n",
    "                                           backpointer=hyp,\n",
    "                                           constraint_index=None,\n",
    "                                           unfinished_constraint=False\n",
    "                                          )\n",
    "            new_hyps.append(new_hyp)\n",
    "        \n",
    "        return new_hyps\n",
    "    \n",
    "    def dumb_generate_from_constraints(self, hyp):\n",
    "        \"\"\"Look at the coverage of the hyp to get constraint candidates\"\"\"\n",
    "        \n",
    "        assert hyp.unfinished_constraint is not True, 'hyp must not be part of an unfinished constraint'\n",
    "        new_constraint_hyps = []\n",
    "        available_constraints = hyp.constraint_candidates()\n",
    "        for idx in available_constraints:\n",
    "            # starting a new constraint\n",
    "            constraint_token = hyp.constraints[idx][0]\n",
    "            # this should come from the model\n",
    "            score = np.random.random()\n",
    "            coverage = copy.deepcopy(hyp.coverage)\n",
    "            coverage[idx][0] = 1\n",
    "            if len(coverage[idx]) > 1:\n",
    "                unfinished_constraint = True\n",
    "            else:\n",
    "                unfinished_constraint = False\n",
    "                \n",
    "            new_hyp = ConstraintHypothesis(token=constraint_token,\n",
    "                                           score=score,\n",
    "                                           coverage=coverage,\n",
    "                                           constraints=hyp.constraints,\n",
    "                                           payload=None,\n",
    "                                           backpointer=hyp,\n",
    "                                           constraint_index=(idx, 0),\n",
    "                                           unfinished_constraint=unfinished_constraint\n",
    "                                          )\n",
    "            new_constraint_hyps.append(new_hyp)\n",
    "        \n",
    "        return new_constraint_hyps\n",
    "        \n",
    "    \n",
    "    def dumb_continue_unfinished_constraint(self, hyp):\n",
    "        assert hyp.unfinished_constraint is True, 'hyp must be part of an unfinished constraint'\n",
    "        \n",
    "        # this should come from the model\n",
    "        score = np.random.random()\n",
    "        \n",
    "        constraint_row_index = hyp.constraint_index[0]\n",
    "        # the index of the next token in the constraint\n",
    "        constraint_tok_index = hyp.constraint_index[1] + 1\n",
    "        constraint_index = (constraint_row_index, constraint_tok_index)\n",
    "        \n",
    "        continued_constraint_token = hyp.constraints[constraint_index[0]][constraint_index[1]]\n",
    "        \n",
    "        coverage = copy.deepcopy(hyp.coverage)\n",
    "        coverage[constraint_row_index][constraint_tok_index] = 1\n",
    "        \n",
    "        if len(hyp.constraints[constraint_row_index]) > constraint_tok_index + 1:\n",
    "            unfinished_constraint = True\n",
    "        else:\n",
    "            unfinished_constraint = False\n",
    "        \n",
    "        new_hyp = ConstraintHypothesis(token=continued_constraint_token,\n",
    "                                       score=score,\n",
    "                                       coverage=coverage,\n",
    "                                       constraints=hyp.constraints,\n",
    "                                       payload=None,\n",
    "                                       backpointer=hyp,\n",
    "                                       constraint_index=constraint_index,\n",
    "                                       unfinished_constraint=unfinished_constraint\n",
    "                                      )\n",
    "        return new_hyp\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_TOKEN = u'<S>'\n",
    "P_START = 1.0\n",
    "N_BEST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dumb_tm = DumbTranslationModel(vocabulary)\n",
    "start_hyp = ConstraintHypothesis(token=START_TOKEN, score=P_START,\n",
    "                                 coverage=init_coverage(sample_constraints),\n",
    "                                 constraints=sample_constraints,\n",
    "                                 payload=None,\n",
    "                                 backpointer=None,\n",
    "                                 unfinished_constraint=False\n",
    "                                )\n",
    "\n",
    "next_hyps = dumb_tm.dumb_generate(start_hyp, n_best=N_BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_new_constraint_hyps = dumb_tm.dumb_generate_from_constraints(start_hyp)\n",
    "len(next_new_constraint_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_continued_constraint_hyps = [dumb_tm.dumb_continue_unfinished_constraint(hyp)\n",
    "                                  for hyp in next_new_constraint_hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_continued_constraint_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(next_continued_constraint_hyps[1].unfinished_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(next_new_constraint_hyps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_next_hyps = [dumb_decoder.dumb_generate(h, n_best=N_BEST) for h in next_hyps]\n",
    "next_next_hyps = [h for s in next_next_hyps for h in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(next_next_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[h.get_sequence() for h in next_next_hyps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(next_hyps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#     def __init__(self, token, score, coverage, constraints, payload=None, backpointer=None,\n",
    "#                  unfinished_constraint=False):\n",
    "start_hyp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hyp(previous_hyp, next_token=None, constraint_idx=None, constraint_token_idx=None)\n",
    "    \"\"\"Utility function to create a hypothesis in different ways\"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
